# -*- coding: utf-8 -*-
"""Copy of Assignment 10. / Project 01. 01.02. Dataset | Nexus | RezaShokrzad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gg7E1D5tiXYdLsQvPqN-iKdxG7jPjan0

# ğŸ· Mini-Project: Merge & Explore the Wine Quality Datasets


> **Goal: Merge Wine Quality â€“ Red and Wine Quality â€“ White (UCI) into a single dataset, do a careful first-look exploration, and save the merged file to CSV.**

<p align="center">ğŸ“¢âš ï¸ğŸ“‚  </p>

<p align="center"> Please name your file using the format: <code>assignmentName_nickname.py/.ipynb</code> (e.g., <code>project1_ali.py</code>) and push it to GitHub with a clear commit message.</p>

<p align="center"> ğŸš¨ğŸ“ğŸ§  </p>
"""

# === Requirements ===
# pip install pandas matplotlib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# ---------- 1) Load ----------
URL_RED   = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
URL_WHITE = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

red   = pd.read_csv(URL_RED, sep=";")
white = pd.read_csv(URL_WHITE, sep=";")

# ---------- 2) Sanity checks ----------
print("Red shape:", red.shape, "White shape:", white.shape)
print("Columns equal? ->", list(red.columns) == list(white.columns))
print("Columns:", list(red.columns))
print("Columns:", list(white.columns))

# (Optional) strict schema assertion (search and read about assert in Python)
assert list(red.columns) == list(white.columns), "Column mismatch between red and white datasets."

"""This is not adding a dictionary to the dataframe, but rather adding a new column called "type" to the red dataframe and setting the value to "red" for all rows.

The analogy is that a dataframe is like an Excel table:

Columns = dictionary keys (column names)

Each column is a Series (a list of values).

So in terms of structure it looks a bit like a dictionary (key â†’ column) but a DataFrame is actually a two-dimensional data structure, not a regular dictionary.

If you want to say:

In a dictionary: dict["key"] = value â†’ stores a key-value pair.

In Pandas: df["column_name"] = value â†’ creates a new column or updates an existing column.

red["type"] = "red"
"""

# ---------- 3) Tag source & merge ----------
red["type"] = "red"        # Gives all rows of the red dataframe the value 'red'
white["type"] = "white"    # Gives all rows of the red dataframe the value 'white'

df = pd.concat([red, white], ignore_index=True)
print(df.head())
print("\nMerged shape:", df.shape)

"""It has two important parts:

\n â†’ is a newline character. This means it creates a blank line before the text is printed. This is only used to make the output neater on the terminal.

df.shape â†’ is an attribute of the Pandas dataframe that returns the size of the table as a tuple:

First number = number of rows

Second number = number of columns

You don't have two type columns because each dataframe (red and white) actually has a separate column,

but when you merge them with pd.concat, Pandas will put the columns together based on their names.

So here's what happens:

You add a column type = "red" to red.

You add a column type = "white" to white.

When concat, Pandas says:

I have a column called type that exists in both dataframes, so instead of making two separate columns, I'm going to merge them together, and only the values will be different.
"""

# ---------- 4) Basic exploration ----------
print("\nDtypes:\n", df.dtypes)
print("\nMissing values per column:\n", df.isnull().sum().sort_values(ascending=False))
print("\nHead:\n", df.head())

"""df.isnull() â†’ For each cell in the dataframe, it checks whether the value is empty (NaN). The output is a boolean dataframe (True/False).

.sum() â†’ Since in Python True = 1 and False = 0, you get the number of NaN values in each column by summing them.

.sort_values(ascending=False) â†’ Sorts the columns by the number of missing values, from highest to lowest.

ğŸ“Œ This line basically tells you how many missing values you have in each column, sorted from highest to lowest.

df.isnull().sum().sort_values(ascending=False) --> The output is in the form of a Pandas Series with the column names as indices and the number of empty values in those columns as their values.
The residual_sugar column has **ten** blank values

The sulphates column has **five** blank values

The other columns are not blank

So each column gets a number indicating **the number of NaNs** in that column.
"""

# Uniqueness & duplicates
dup_count = df.duplicated().sum()     # Numer of exactly duplicate rows
print("\nDuplicate rows:", dup_count)

# Descriptive statistics (numeric)
num_cols = df.select_dtypes(include=[np.number]).columns # The output of num_cols is an Index object (similar to a list, but specific to Pandas).
print("\nNumeric summary:\n", df[num_cols].describe().T)

# Target distributions
print("\nQuality distribution (overall):\n", df["quality"].value_counts().sort_index())               #df["quality"] â†’ Selects the quality column from the dataframe.
#.value_counts() â†’ Counts the number of occurrences of each unique value in the quality column.
#.sort_index() â†’ Sorts the results by the value of the quality itself (not its count).
print("\nQuality distribution by type:\n", df.groupby("type")["quality"].value_counts().sort_index()) #ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø± Ù…Ù‚Ø¯Ø§Ø± quality Ø¯Ø± Ù‡Ø± Ú¯Ø±ÙˆÙ‡ Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù…Ø§Ø±Ø¯.

"""df["quality"] â†’ Selects the quality column from the dataframe.

.value_counts() â†’ Counts the number of occurrences of each unique value in the quality column.

.sort_index() â†’ Sorts the results by the value of the quality itself (not its count).

df["quality"] â†’ Selects the quality column from the dataframe.

.value_counts() â†’ Counts the number of occurrences of each unique value in the quality column.

.sort_index() â†’ Sorts the results by the value of the quality itself (not its count).

df.duplicated().sum() â†’ Number of exactly duplicate rows.

df[num_cols].describe().T â†’ Basic statistics (mean, std, min, max, etc.) for numeric columns.

df["quality"].value_counts().sort_index() â†’ Qualitative distribution of the entire data.

df.groupby("type")["quality"].value_counts().sort_index() â†’ Qualitative distribution by type (red/white).

select_dtypes(include=[np.number]) says: "Select only columns whose data type is number."

In Pandas, np.number includes **int** and **float**.

Columns like fixed acidity, residual sugar, and pH are numeric, but type is text, so it is omitted.
sort_values(): Sorts by the number or value in the column.
sort_index(): Sorts by the **index value** itself (here, the numbers 3,4,5,6...).
"""

#Correlation matrix between all numeric columns.
#['target'] â†’ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù‡Ø± Ø³ØªÙˆÙ† Ø¨Ø§ Ù‡Ø¯Ù.
#We just take the absolute value of the correlation, whether positive or negative. corr_matrix['quality'].abs()
#sort_values(ascending=False) â†’ Sort from highest to lowest.
#index[1:5] â†’ [0] is the target column itself, so we start at 1, up to the next 4 columns.
#.tolist() â†’ Converts to a list.
print(df.head())
num_col=df.select_dtypes(include=[np.number]).columns
corr_matrix = df[num_col].corr()
top_vars = corr_matrix['quality'].abs().sort_values(ascending=False).index[1:5].tolist()
print(top_vars)

# ---------- 5) A few simple visuals (optional for report) ----------
# Histograms of numeric features (quick feel for ranges & skew)

n = min(4, len(top_vars))

#plt.subplots(1, 4):This function is used by the Matplotlib library to create a page with multiple subplots.
#sharey=True: The Y axis (numbers or values) is shared between all subcharts. This makes it easier to compare bar heights between charts.

fig, axes = plt.subplots(1, 4, figsize=(16, 3), sharey=True)

#axes: axes is the same array of 4 subplots as previously created with plt.subplots(1, 4). Each element is an axis (Axes) on which to plot.
#enumerate(axes): The enumerate function returns two things: The index of the element (i) â†’ i.e. 0, 1, 2, 3. The element itself (ax) â†’ each of the axes i.e. i is the subplot number and ax is the axis on which we plot the histogram.
for i, ax in enumerate(axes):
    if i < n:
        col = top_vars[i]
        ax.hist(df[col].dropna(), bins=30) #ax.hist is the command to draw a histogram on the current subplot. df[col].dropna() â†’ data from the selected column, without null values (NaN).
        ax.set_title(f"Histogram: {col}")
        ax.set_xlabel(col)
        if i == 0:
            ax.set_ylabel("Count")
        else:
            ax.set_ylabel("")
    else:
        ax.axis("off")  # hide unused panels if top_vars has < 4

plt.tight_layout()
plt.show()

# Boxplot of quality by type (class distribution spread)
plt.figure()
df.boxplot(column="quality", by="type")
plt.suptitle("")
plt.title("Quality by Wine Type")
plt.xlabel("Type")
plt.ylabel("Quality")
plt.tight_layout()
plt.show()

# Correlation heatmap (numeric only)
corr = df[num_cols].corr()
plt.figure(figsize=(7, 6))
#corr â†’ Correlation matrix previously created with df.corr() or similar. plt.imshow() â† Displays 2D data (matrix) as an image.
plt.imshow(corr, interpolation="nearest")
plt.title("Correlation Heatmap")
plt.colorbar()
plt.xticks(range(len(num_cols)), num_cols, rotation=90)
plt.yticks(range(len(num_cols)), num_cols)
plt.tight_layout()
plt.show()

# ---------- 6) Save ----------

#Ø³Ø§Ø®Øª Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ (Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯Ù†):

#OUT_DIR is the path to the folder where you want to save the CSV file.
OUT_DIR = Path("./outputs")

#parents=True â†’ If the parent folders do not exist, they will be created. exist_ok=True â†’ If the folder already exists, no error will be thrown.
OUT_DIR.mkdir(parents=True, exist_ok=True)

#This line specifies the path to the CSV file in the OUT_DIR folder. For example, if OUT_DIR = Path("./outputs"), the file path would be: outputs/wine_quality_merged.csv.
out_file = OUT_DIR / "wine_quality_merged.csv"

#The data in df is stored in the file whose path is specified in out_file. index=False means that the row numbers of the DataFrame are not stored in the CSV.
df.to_csv(out_file, index=False)
#out_file.resolve() returns the absolute path of the file, i.e. where it is actually stored on disk. The output might look like this:
#Saved merged file to: /home/user/project/outputs/wine_quality_merged.csv
print(f"\nSaved merged file to: {out_file.resolve()}")

# Quick verification of saved file
#These two lines of code are used to quickly check the saved CSV file: Read the CSV file back into the DataFrame. Check the dimensions of the loaded DataFrame.
df_check = pd.read_csv(out_file)
print("Reloaded shape:", df_check.shape)

"""This code you wrote saves a **DataFrame** (df) to disk and reloads it to make sure the file was saved correctly. Creates a folder called outputs in the current path.

parents=True â†’ If the parent folders do not exist, they will be created.

exist_ok=True â†’ If the folder already exists, it will not throw an error.
"""