# -*- coding: utf-8 -*-
"""Copy of Assignment 10. / Project 01. 01.02. Dataset | Nexus | RezaShokrzad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gg7E1D5tiXYdLsQvPqN-iKdxG7jPjan0

# 🍷 Mini-Project: Merge & Explore the Wine Quality Datasets


> **Goal: Merge Wine Quality – Red and Wine Quality – White (UCI) into a single dataset, do a careful first-look exploration, and save the merged file to CSV.**

<p align="center">📢⚠️📂  </p>

<p align="center"> Please name your file using the format: <code>assignmentName_nickname.py/.ipynb</code> (e.g., <code>project1_ali.py</code>) and push it to GitHub with a clear commit message.</p>

<p align="center"> 🚨📝🧠 </p>
"""

# === Requirements ===
# pip install pandas matplotlib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# ---------- 1) Load ----------
URL_RED   = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
URL_WHITE = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"

red   = pd.read_csv(URL_RED, sep=";")
white = pd.read_csv(URL_WHITE, sep=";")

# ---------- 2) Sanity checks ----------
print("Red shape:", red.shape, "White shape:", white.shape)
print("Columns equal? ->", list(red.columns) == list(white.columns))
print("Columns:", list(red.columns))
print("Columns:", list(white.columns))

# (Optional) strict schema assertion (search and read about assert in Python)
assert list(red.columns) == list(white.columns), "Column mismatch between red and white datasets."

"""This is not adding a dictionary to the dataframe, but rather adding a new column called "type" to the red dataframe and setting the value to "red" for all rows.

The analogy is that a dataframe is like an Excel table:

Columns = dictionary keys (column names)

Each column is a Series (a list of values).

So in terms of structure it looks a bit like a dictionary (key → column) but a DataFrame is actually a two-dimensional data structure, not a regular dictionary.

If you want to say:

In a dictionary: dict["key"] = value → stores a key-value pair.

In Pandas: df["column_name"] = value → creates a new column or updates an existing column.

red["type"] = "red"
"""

# ---------- 3) Tag source & merge ----------
red["type"] = "red"        # Gives all rows of the red dataframe the value 'red'
white["type"] = "white"    # Gives all rows of the red dataframe the value 'white'

df = pd.concat([red, white], ignore_index=True)
print(df.head())
print("\nMerged shape:", df.shape)

"""It has two important parts:

\n → is a newline character. This means it creates a blank line before the text is printed. This is only used to make the output neater on the terminal.

df.shape → is an attribute of the Pandas dataframe that returns the size of the table as a tuple:

First number = number of rows

Second number = number of columns

You don't have two type columns because each dataframe (red and white) actually has a separate column,

but when you merge them with pd.concat, Pandas will put the columns together based on their names.

So here's what happens:

You add a column type = "red" to red.

You add a column type = "white" to white.

When concat, Pandas says:

I have a column called type that exists in both dataframes, so instead of making two separate columns, I'm going to merge them together, and only the values will be different.
"""

# ---------- 4) Basic exploration ----------
print("\nDtypes:\n", df.dtypes)
print("\nMissing values per column:\n", df.isnull().sum().sort_values(ascending=False))
print("\nHead:\n", df.head())

"""df.isnull() → For each cell in the dataframe, it checks whether the value is empty (NaN). The output is a boolean dataframe (True/False).

.sum() → Since in Python True = 1 and False = 0, you get the number of NaN values in each column by summing them.

.sort_values(ascending=False) → Sorts the columns by the number of missing values, from highest to lowest.

📌 This line basically tells you how many missing values you have in each column, sorted from highest to lowest.

df.isnull().sum().sort_values(ascending=False) --> The output is in the form of a Pandas Series with the column names as indices and the number of empty values in those columns as their values.
The residual_sugar column has **ten** blank values

The sulphates column has **five** blank values

The other columns are not blank

So each column gets a number indicating **the number of NaNs** in that column.
"""

# Uniqueness & duplicates
dup_count = df.duplicated().sum()     # Numer of exactly duplicate rows
print("\nDuplicate rows:", dup_count)

# Descriptive statistics (numeric)
num_cols = df.select_dtypes(include=[np.number]).columns # The output of num_cols is an Index object (similar to a list, but specific to Pandas).
print("\nNumeric summary:\n", df[num_cols].describe().T)

# Target distributions
print("\nQuality distribution (overall):\n", df["quality"].value_counts().sort_index())               #df["quality"] → Selects the quality column from the dataframe.
#.value_counts() → Counts the number of occurrences of each unique value in the quality column.
#.sort_index() → Sorts the results by the value of the quality itself (not its count).
print("\nQuality distribution by type:\n", df.groupby("type")["quality"].value_counts().sort_index()) #تعداد هر مقدار quality در هر گروه را می‌شمارد.

"""df["quality"] → Selects the quality column from the dataframe.

.value_counts() → Counts the number of occurrences of each unique value in the quality column.

.sort_index() → Sorts the results by the value of the quality itself (not its count).

df["quality"] → Selects the quality column from the dataframe.

.value_counts() → Counts the number of occurrences of each unique value in the quality column.

.sort_index() → Sorts the results by the value of the quality itself (not its count).

df.duplicated().sum() → Number of exactly duplicate rows.

df[num_cols].describe().T → Basic statistics (mean, std, min, max, etc.) for numeric columns.

df["quality"].value_counts().sort_index() → Qualitative distribution of the entire data.

df.groupby("type")["quality"].value_counts().sort_index() → Qualitative distribution by type (red/white).

select_dtypes(include=[np.number]) says: "Select only columns whose data type is number."

In Pandas, np.number includes **int** and **float**.

Columns like fixed acidity, residual sugar, and pH are numeric, but type is text, so it is omitted.
sort_values(): Sorts by the number or value in the column.
sort_index(): Sorts by the **index value** itself (here, the numbers 3,4,5,6...).
"""

#Correlation matrix between all numeric columns.
#['target'] → همبستگی هر ستون با هدف.
#We just take the absolute value of the correlation, whether positive or negative. corr_matrix['quality'].abs()
#sort_values(ascending=False) → Sort from highest to lowest.
#index[1:5] → [0] is the target column itself, so we start at 1, up to the next 4 columns.
#.tolist() → Converts to a list.
print(df.head())
num_col=df.select_dtypes(include=[np.number]).columns
corr_matrix = df[num_col].corr()
top_vars = corr_matrix['quality'].abs().sort_values(ascending=False).index[1:5].tolist()
print(top_vars)

# ---------- 5) A few simple visuals (optional for report) ----------
# Histograms of numeric features (quick feel for ranges & skew)

n = min(4, len(top_vars))

#plt.subplots(1, 4):This function is used by the Matplotlib library to create a page with multiple subplots.
#sharey=True: The Y axis (numbers or values) is shared between all subcharts. This makes it easier to compare bar heights between charts.

fig, axes = plt.subplots(1, 4, figsize=(16, 3), sharey=True)

#axes: axes is the same array of 4 subplots as previously created with plt.subplots(1, 4). Each element is an axis (Axes) on which to plot.
#enumerate(axes): The enumerate function returns two things: The index of the element (i) → i.e. 0, 1, 2, 3. The element itself (ax) → each of the axes i.e. i is the subplot number and ax is the axis on which we plot the histogram.
for i, ax in enumerate(axes):
    if i < n:
        col = top_vars[i]
        ax.hist(df[col].dropna(), bins=30) #ax.hist is the command to draw a histogram on the current subplot. df[col].dropna() → data from the selected column, without null values (NaN).
        ax.set_title(f"Histogram: {col}")
        ax.set_xlabel(col)
        if i == 0:
            ax.set_ylabel("Count")
        else:
            ax.set_ylabel("")
    else:
        ax.axis("off")  # hide unused panels if top_vars has < 4

plt.tight_layout()
plt.show()

# Boxplot of quality by type (class distribution spread)
plt.figure()
df.boxplot(column="quality", by="type")
plt.suptitle("")
plt.title("Quality by Wine Type")
plt.xlabel("Type")
plt.ylabel("Quality")
plt.tight_layout()
plt.show()

# Correlation heatmap (numeric only)
corr = df[num_cols].corr()
plt.figure(figsize=(7, 6))
#corr → Correlation matrix previously created with df.corr() or similar. plt.imshow() ← Displays 2D data (matrix) as an image.
plt.imshow(corr, interpolation="nearest")
plt.title("Correlation Heatmap")
plt.colorbar()
plt.xticks(range(len(num_cols)), num_cols, rotation=90)
plt.yticks(range(len(num_cols)), num_cols)
plt.tight_layout()
plt.show()

# ---------- 6) Save ----------

#ساخت پوشه خروجی (در صورت نبودن):

#OUT_DIR is the path to the folder where you want to save the CSV file.
OUT_DIR = Path("./outputs")

#parents=True → If the parent folders do not exist, they will be created. exist_ok=True → If the folder already exists, no error will be thrown.
OUT_DIR.mkdir(parents=True, exist_ok=True)

#This line specifies the path to the CSV file in the OUT_DIR folder. For example, if OUT_DIR = Path("./outputs"), the file path would be: outputs/wine_quality_merged.csv.
out_file = OUT_DIR / "wine_quality_merged.csv"

#The data in df is stored in the file whose path is specified in out_file. index=False means that the row numbers of the DataFrame are not stored in the CSV.
df.to_csv(out_file, index=False)
#out_file.resolve() returns the absolute path of the file, i.e. where it is actually stored on disk. The output might look like this:
#Saved merged file to: /home/user/project/outputs/wine_quality_merged.csv
print(f"\nSaved merged file to: {out_file.resolve()}")

# Quick verification of saved file
#These two lines of code are used to quickly check the saved CSV file: Read the CSV file back into the DataFrame. Check the dimensions of the loaded DataFrame.
df_check = pd.read_csv(out_file)
print("Reloaded shape:", df_check.shape)

"""This code you wrote saves a **DataFrame** (df) to disk and reloads it to make sure the file was saved correctly. Creates a folder called outputs in the current path.

parents=True → If the parent folders do not exist, they will be created.

exist_ok=True → If the folder already exists, it will not throw an error.
"""